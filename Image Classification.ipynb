{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SID500344127_code .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g015B3-HtT_B"
      },
      "source": [
        "\n",
        "\n",
        "Import libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iF52B3OWiJ"
      },
      "source": [
        "import h5py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "import time\n",
        "from scipy.optimize import minimize,fmin_tnc,fmin,fmin_l_bfgs_b,fmin_cobyla,fmin_slsqp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXKF9BXWOJkS",
        "outputId": "02d7fcc0-1ded-41e7-d784-6eee4387b821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My\\ Drive/\n",
        "!pwd\n",
        "\n",
        "with h5py.File('images_training.h5','r') as H:\n",
        "    data_train = np.copy(H['datatrain'])\n",
        "with h5py.File('labels_training.h5','r') as H:\n",
        "    label_train = np.copy(H['labeltrain'])\n",
        "with h5py.File('images_testing.h5','r') as H:\n",
        "    data_test = np.copy(H['datatest'])\n",
        "with h5py.File('labels_testing_2000.h5','r') as H:\n",
        "    label_test = np.copy(H['labeltest'])\n",
        "print(data_train.shape)\n",
        "print(label_train.shape)\n",
        "print(data_test.shape)\n",
        "print(label_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive\n",
            "/content/gdrive/My Drive\n",
            "(30000, 784)\n",
            "(30000,)\n",
            "(5000, 784)\n",
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEyOQVj9N5wU"
      },
      "source": [
        "Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkbVdkVpDzMn",
        "outputId": "687a3ee7-2edc-4dc7-e1fa-dfdd5e8c18fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(os.listdir(\"./Input/test\"))\n",
        "print(os.listdir(\"./Input/train\"))\n",
        "#Loading training data\n",
        "with h5py.File('./Input/train/images_training.h5','r') as H:\n",
        "    data_train = np.copy(H['datatrain'])\n",
        "with h5py.File('./Input/train/labels_training.h5','r') as H:\n",
        "    label_train = np.copy(H['labeltrain'])\n",
        "print(data_train.shape)\n",
        "print(label_train.shape)\n",
        "\n",
        "#Loading testing data\n",
        "with h5py.File('./Input/test/images_testing.h5','r') as H:\n",
        "    data_test = np.copy(H['datatest'])\n",
        "with h5py.File('./Input/test/labels_testing_2000.h5','r') as H:\n",
        "    label_test = np.copy(H['labeltest'])\n",
        "print(data_test.shape)\n",
        "print(label_test.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['labels_testing_2000.h5', 'images_testing.h5']\n",
            "['images_training.h5', 'labels_training.h5']\n",
            "(30000, 784)\n",
            "(30000,)\n",
            "(5000, 784)\n",
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMhESjl0uDLH"
      },
      "source": [
        "Assign to variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv28yaSyS3GF",
        "outputId": "820f69ff-97a1-40a2-d60d-f80c79d61e6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "training_data = data_train\n",
        "testing_data = data_test\n",
        "training_label = label_train\n",
        "testing_label = label_test\n",
        "\n",
        "print(training_data.shape)\n",
        "print(training_label.shape)\n",
        "print(testing_data.shape)\n",
        "print(testing_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 784)\n",
            "(30000,)\n",
            "(5000, 784)\n",
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ozGONVvN3Kj"
      },
      "source": [
        "Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdvzRizcN2lw",
        "outputId": "105e749c-fb4e-4390-fc12-dc06c1a4eb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Function to scale training data \n",
        "def standardize(array):\n",
        "  x = array \n",
        "  x-=np.mean(x)\n",
        "  x/=np.std(x)\n",
        "  standardize_array = x\n",
        "  return standardize_array\n",
        "# Function to scale training data \n",
        "def standardize_test(array,training_data):\n",
        "  x = array \n",
        "  x-=np.mean(training_data)\n",
        "  x/=np.std(training_data)\n",
        "  standardize_array = x\n",
        "  return standardize_array\n",
        "\n",
        "# Function to reduce the dimensions of data using PCA \n",
        "def pca_train(data):\n",
        "  # Covariance matrix\n",
        "  covariance_matrix = np.cov(data.T)\n",
        "  # print(covariance_matrix)\n",
        "\n",
        "  # Eigen Value decomposition \n",
        "  eigen_values, eigen_vectors = np.linalg.eig(covariance_matrix)\n",
        "  # print(\"Eigenvector: \\n\",eigen_vectors,\"\\n\")\n",
        "  # print(\"Eigenvalues: \\n\", eigen_values, \"\\n\")\n",
        "  # print(eigen_values.shape)\n",
        "  # print(eigen_vectors.shape) #principal components \n",
        "\n",
        "  # Variance calculation \n",
        "  variance_explained = []\n",
        "  for i in eigen_values:\n",
        "      variance_explained.append((i/sum(eigen_values))*100)\n",
        "  cumulative_variance_explained = np.cumsum(variance_explained)\n",
        "  np.around(cumulative_variance_explained,2,out=cumulative_variance_explained)\n",
        "  # plt.plot(range(1,eigen_values.shape[0]+1),cumulative_variance_explained)\n",
        "  # plt.xlabel(\"Number of components\")\n",
        "  # plt.ylabel(\"Cumulative explained variance\")\n",
        "  # plt.title(\"Explained variance vs Number of components\")\n",
        "  \n",
        "  # Feature selection \n",
        "  no_of_features = np.where(cumulative_variance_explained > 95)[0][0]\n",
        "  # print(no_of_features)\n",
        "\n",
        "  # Projection matrix using the features\n",
        "  projection_matrix = (eigen_vectors.T[:][:no_of_features]).T\n",
        "  # print(projection_matrix.shape)\n",
        "\n",
        "  # Data with reduced dimensions\n",
        "  data_pca = data.dot(projection_matrix)\n",
        "  # print(data_train_std_pca.shape)\n",
        "\n",
        "  return data_pca,projection_matrix\n",
        "\n",
        "def pca_test(data,projection_matrix):\n",
        "  # Data with reduced dimensions\n",
        "  data_pca = data.dot(projection_matrix)\n",
        "  # print(data_train_std_pca.shape)\n",
        "  return data_pca\n",
        "\n",
        "# Function to preprocess training data\n",
        "def preprocessing_training_data(data_train):\n",
        "  #standardizing \n",
        "  data_train_std = standardize(data_train)\n",
        "  #PCA\n",
        "  data_train_std_pca,projection_matrix = pca_train(data_train_std)\n",
        "\n",
        "  return data_train_std_pca,projection_matrix\n",
        "\n",
        "# Function to preprocess testing data\n",
        "def preprocessing_testing_data(data_test,data_train,projection_matrix):\n",
        "  #standardizing \n",
        "  data_test_std = standardize_test(data_test,data_train)\n",
        "  #PCA\n",
        "  data_test_std_pca = pca_test(data_test_std,projection_matrix)\n",
        "  return data_test_std_pca\n",
        "\n",
        "\n",
        "data_train_std_pca,projection_matrix = preprocessing_training_data(training_data)\n",
        "print(data_train_std_pca.shape)\n",
        "\n",
        "data_test_std_pca = preprocessing_testing_data(testing_data,training_data,projection_matrix)\n",
        "print(data_test_std_pca.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 187)\n",
            "(5000, 187)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fx0201jdPWo",
        "outputId": "9cdc07d1-ad66-472c-bd65-7dab5ccadd2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "training_data = data_train_std_pca\n",
        "testing_data = data_test_std_pca\n",
        "training_label = label_train\n",
        "testing_label = label_test\n",
        "\n",
        "# training_data = data_train_std_pca[:21000]\n",
        "# validation_data = data_test_std_pca[21000:]\n",
        "# training_label = label_train[:21000]\n",
        "# validation_label = label_test[21000:]\n",
        "print(training_data.shape)\n",
        "print(training_label.shape)\n",
        "print(testing_data.shape)\n",
        "print(testing_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 187)\n",
            "(30000,)\n",
            "(5000, 187)\n",
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POaL1aUdcjYX"
      },
      "source": [
        "KNN Classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_Lp-GTAccdq"
      },
      "source": [
        "class knnclassifier:\n",
        "  def __init__(self,train_data,train_label):\n",
        "    self.train_data = train_data \n",
        "    self.train_label = train_label\n",
        "\n",
        "  # Function to calculate the k nearest neighbours and return the mode class from the k nearest neighbours\n",
        "  def knn(self,x_instance,k):\n",
        "    dis = ((self.train_data - x_instance)**2).sum(axis=1)\n",
        "    arg_ascending = np.argsort(dis)\n",
        "    k_neighbours = arg_ascending[:k]\n",
        "    k_neighbours_class = self.train_label[k_neighbours]\n",
        "    u, c = np.unique(k_neighbours_class, return_couns = True)\n",
        "    # print(u)\n",
        "    # print(c)\n",
        "    y = u[c == c.max()]\n",
        "    return y \n",
        "\n",
        "  # Function to predict the labels of the test data\n",
        "  def predict(self,test_data,test_label,k):\n",
        "    self.test_data = test_data\n",
        "    self.test_label = test_label\n",
        "    self.k = k\n",
        "    prediction=[]\n",
        "    for x in range(len(self.test_data)):\n",
        "      pred=self.knn(self.test_data[x],self.k)\n",
        "      # print(pred)\n",
        "      self.train_data=np.append(self.train_data,[self.test_data[x]],axis=0)\n",
        "      self.train_label=np.append(self.train_label,[self.test_label[x]],axis=0)\n",
        "      prediction.append(pred[0])\n",
        "    return prediction\n",
        "\n",
        "# K fold cross validation for the trainining accuracy \n",
        "class k_fold_cross_validation:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  # Function to split data into folds \n",
        "  def split_data(self,data,labels,no_of_folds):\n",
        "    each_fold_size = int(len(data)/no_of_folds)\n",
        "    j = 0\n",
        "    data_after_split = []\n",
        "    labels_after_split = []\n",
        "    for i in range(no_of_folds):\n",
        "      fold = []\n",
        "      fold_labels = []\n",
        "      fold = data[j:(j + each_fold_size)]\n",
        "      fold_labels = labels[j:(j + each_fold_size)]\n",
        "      j += each_fold_size\n",
        "      data_after_split.append(fold)\n",
        "      labels_after_split.append(fold_labels)\n",
        "    return data_after_split,labels_after_split\n",
        "  \n",
        "  # Function to evaluate the knn classifier using cross validation\n",
        "  def evaluate(self,data,labels,no_of_folds,k):\n",
        "    data_folds,label_folds = self.split_data(data,labels,no_of_folds)\n",
        "    # print(len(label_folds))\n",
        "    scores = []\n",
        "    for i in range(no_of_folds):\n",
        "      train_data = data_folds.copy()\n",
        "      train_labels = label_folds.copy()\n",
        "      test_data = data_folds[i]\n",
        "      test_labels = label_folds[i]\n",
        "      del train_data[i]\n",
        "      del train_labels[i]\n",
        "      # print(np.vstack((train_data[0],train_data[1])))\n",
        "      temp_data_array = train_data[0]\n",
        "      temp_label_array = train_labels[0]\n",
        "      for j in range(len(train_data)-1):\n",
        "        temp_data_array = np.vstack((temp_data_array,train_data[j+1]))\n",
        "        temp_label_array = np.vstack((temp_label_array,train_labels[j+1]))\n",
        "        # train_label_stack.append(np.vstack((train_labels[j],train_labels[j+1])))\n",
        "      # final_train_data = train_data_stack\n",
        "      # print(len(temp_data_array))\n",
        "      final_train_labels = np.resize(temp_label_array,(1,len(temp_data_array)))[0]\n",
        "      final_train_data = temp_data_array\n",
        "      # print(type(final_train_data))\n",
        "      # print(type(final_train_labels))\n",
        "      # print(len(final_train_data))\n",
        "      # print(len(final_train_labels))\n",
        "      knn = knnclassifier(final_train_data,final_train_labels)\n",
        "      predictions = knn.predict(test_data,test_labels,k)\n",
        "      accuracy = ((predictions == test_labels).mean())*100\n",
        "      scores.append(accuracy)\n",
        "      # print(scores)\n",
        "    # print('scores:',scores)\n",
        "    # print('max accuracy:',max(scores))\n",
        "    # print('mean accuracy:',sum(scores)/len(scores))\n",
        "    return sum(scores)/len(scores)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfktQpFykVuH"
      },
      "source": [
        "Training knn classifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7T43CFXeqOJ",
        "outputId": "5c841096-c153-4170-b5e6-afa15f0dc920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "start = time.time()\n",
        "kfcv = k_fold_cross_validation()\n",
        "# k_values = []\n",
        "# accuracy_values = []\n",
        "# for k in range(1,20):\n",
        "knn_training_accuracy = kfcv.evaluate(training_data,training_label,10,6)\n",
        "  # k_values.append(k)\n",
        "  # accuracy_values.append(mean_accuracy)\n",
        "end = time.time()\n",
        "knn_training_time = time.strftime(\"%H:%M:%S\", time.gmtime(end-start))\n",
        "print('Training Accuracy:',knn_training_accuracy)\n",
        "print('Training time:',knn_training_time)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 85.55\n",
            "Training time: 00:16:43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQBjfoATe8T1"
      },
      "source": [
        "# knn = knnclassifier(training_data,training_label)\n",
        "# predictions = knn.predict(validation_data,validation_label,6)\n",
        "# validation_accuracy = (predictions == validation_label).mean() * 100\n",
        "\n",
        "# print(validation_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lk3dMEykaDJ"
      },
      "source": [
        "knn Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kuhVC_KfHWY",
        "outputId": "182e50ea-f3f0-44c7-9022-0f73785790d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "start = time.time()\n",
        "knn = knnclassifier(training_data,training_label)\n",
        "testing_data = testing_data[:2000]\n",
        "knn_testing_predictions = knn.predict(testing_data,testing_label,6)\n",
        "testing_predictions = knn_testing_predictions[:2000]\n",
        "knn_testing_accuracy = (testing_predictions == testing_label).mean() * 100\n",
        "end = time.time()\n",
        "print('Testing Accuracy:',knn_testing_accuracy)\n",
        "knn_running_time = time.strftime(\"%H:%M:%S\", time.gmtime(end-start))\n",
        "print('Running time:',knn_running_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 73.0\n",
            "Running time: 00:01:11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEoGvXSbz7r0",
        "outputId": "8a0985f9-d626-4f58-c230-f8e3c40c238e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "training_data = data_train_std_pca\n",
        "testing_data = data_test_std_pca\n",
        "training_label = label_train\n",
        "testing_label = label_test\n",
        "\n",
        "print(training_data.shape)\n",
        "print(training_label.shape)\n",
        "print(testing_data.shape)\n",
        "print(testing_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 187)\n",
            "(30000,)\n",
            "(5000, 187)\n",
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9IcQhEckRqm"
      },
      "source": [
        "Naive Bayes "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DloJ3UlxkT10"
      },
      "source": [
        "class cross_validation:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  # Function to split data into folds \n",
        "  def split_data(self,data,labels,no_of_folds):\n",
        "    each_fold_size = int(len(data)/no_of_folds)\n",
        "    j = 0\n",
        "    data_after_split = []\n",
        "    labels_after_split = []\n",
        "    for i in range(no_of_folds):\n",
        "      fold = []\n",
        "      fold_labels = []\n",
        "      fold = data[j:(j + each_fold_size)]\n",
        "      fold_labels = labels[j:(j + each_fold_size)]\n",
        "      j += each_fold_size\n",
        "      data_after_split.append(fold)\n",
        "      labels_after_split.append(fold_labels)\n",
        "    return data_after_split,labels_after_split\n",
        "\n",
        "  # Function to evaluate and calculate the training accuracy using cross validation\n",
        "  def evaluate(self,data,labels,no_of_folds):\n",
        "    data_folds,label_folds = self.split_data(data,labels,no_of_folds)\n",
        "    # print(len(label_folds))\n",
        "    scores = []\n",
        "    for i in range(no_of_folds):\n",
        "      train_data = data_folds.copy()\n",
        "      train_labels = label_folds.copy()\n",
        "      test_data = data_folds[i]\n",
        "      test_labels = label_folds[i]\n",
        "      del train_data[i]\n",
        "      del train_labels[i]\n",
        "      # print(np.vstack((train_data[0],train_data[1])))\n",
        "      temp_data_array = train_data[0]\n",
        "      temp_label_array = train_labels[0]\n",
        "      for j in range(len(train_data)-1):\n",
        "        temp_data_array = np.vstack((temp_data_array,train_data[j+1]))\n",
        "        temp_label_array = np.vstack((temp_label_array,train_labels[j+1]))\n",
        "        # train_label_stack.append(np.vstack((train_labels[j],train_labels[j+1])))\n",
        "      # final_train_data = train_data_stack\n",
        "      # print(len(temp_data_array))\n",
        "      final_train_labels = np.resize(temp_label_array,(1,len(temp_data_array)))[0]\n",
        "      final_train_data = temp_data_array\n",
        "      # print(type(final_train_data))\n",
        "      # print(type(final_train_labels))\n",
        "      # print(len(final_train_data))\n",
        "      # print(len(final_train_labels))\n",
        "      predictions = naive_bayes(final_train_data,final_train_labels,test_data,test_labels)\n",
        "      accuracy = ((predictions == test_labels).mean())*100\n",
        "      scores.append(accuracy)\n",
        "      # print(scores)\n",
        "    # print('scores:',scores)\n",
        "    # print('max accuracy:',max(scores))\n",
        "    # print('mean accuracy:',sum(scores)/len(scores))\n",
        "    return sum(scores)/len(scores)\n",
        "\n",
        "# Function to split data by class values and return a dictionary of class as keys and data as respective values\n",
        "def separate_class(x_train,y_train) :\n",
        "  dict1 = {}\n",
        "  for i in range(len(x_train)):\n",
        "    \n",
        "    class_label = y_train[i]\n",
        "    if class_label not in dict1:\n",
        "      dict1[class_label] = list()\n",
        "    dict1[class_label].append(x_train[i])\n",
        "  return dict1\n",
        "\n",
        "# Function to calculate prior probabilities and the mean and variance of each feature of each class\n",
        "def statistics(class_dict):\n",
        "  classes=list()\n",
        "  number_in_each_class = list()\n",
        "  for key,value in class_dict.items():\n",
        "    classes.append(key)\n",
        "    number_in_each_class.append(len(value))\n",
        "  # print(classes)\n",
        "  # print(number_in_each_class)\n",
        "  class_wise_numbers = [x for _,x in sorted(zip(classes,number_in_each_class))]\n",
        "  # print('class wise numbers:',class_wise_numbers)\n",
        "  classes_sorted = sorted(classes)\n",
        "  # print('classes:',classes_sorted)\n",
        "  total = np.sum(np.array(class_wise_numbers))\n",
        "  # print(total)\n",
        "  prior_probabilities = np.array(class_wise_numbers)/total\n",
        "  # print(prior_probabilities)\n",
        "\n",
        "  #Likelihood \n",
        "  class_means = {}\n",
        "  class_var = {}\n",
        "  for key,value in class_dict.items():\n",
        "    means = []\n",
        "    var = []\n",
        "    for column in zip(*value):\n",
        "      means.append(np.mean(column))\n",
        "      var.append(np.var(column)+1e-3)\n",
        "    class_means[key] = means\n",
        "    class_var[key] = var\n",
        "  class_means_sorted = dict(sorted(class_means.items()))\n",
        "  class_var_sorted = dict(sorted(class_var.items()))\n",
        "  stats = {}\n",
        "  # count = 0 \n",
        "  for value in classes_sorted:\n",
        "    # count +=1\n",
        "    temp_list = []\n",
        "    for i in range(187):\n",
        "      temp_tuple = ()\n",
        "      temp_tuple = (class_means_sorted[value][i],) + (class_var_sorted[value][i],) \n",
        "      temp_list.append(temp_tuple)\n",
        "    stats[value] = temp_list\n",
        "  # print(count)\n",
        "  return stats, prior_probabilities\n",
        "\n",
        "# Function to calculate the gaussian distribution probability\n",
        "def probability(sample,mean,variance):\n",
        "    p = 1/(np.sqrt(2*np.pi*variance)) * np.exp((-(sample-mean)**2)/(2*variance))\n",
        "    return p\n",
        "# Function to predict the probability of belonging to a class given the features\n",
        "def class_probability(row, stats,prior_probabilities):\n",
        "  # total_rows = sum([stats[label][0][2] for label in stats])\n",
        "  probabilities = {}\n",
        "  # count = 0 \n",
        "  for key, value in stats.items():\n",
        "\t\t# probabilities[key] = stats[key][0][2]/float(total_rows) # prior probability \n",
        "    probabilities[key] = prior_probabilities[key]\n",
        "    for i in range(len(value)):\n",
        "      mean, var = stats[key][i]\n",
        "      probabilities[key] *= probability(row[i], mean, var)\n",
        "    # count +=1\n",
        "  return probabilities\n",
        "\n",
        "# Function to Predict the best class for a given row\n",
        "def predict(stats,row,prior_probabilities):\n",
        "  probabilities = class_probability(row,stats,prior_probabilities)\n",
        "  values = list(probabilities.values())\n",
        "  keys = list(probabilities.keys())\n",
        "  best_class =  keys[values.index(max(values))]\n",
        "  return best_class\n",
        "\n",
        "\n",
        "def naive_bayes(train, y_train,test,y_test):\n",
        "  all_dictionary = separate_class(train,y_train)\n",
        "  stats,prior_probabilities = statistics(all_dictionary)\n",
        "  # print(stats)\n",
        "  predictions = list()\n",
        "  for row in test:\n",
        "    output = predict(stats, row,prior_probabilities)\n",
        "    predictions.append(output)\n",
        "  return predictions\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNcjWZ_FsSam"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KuEXd_frDdd",
        "outputId": "92301fcd-2b3c-48fc-c042-948817090c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "start = time.time()\n",
        "cv = cross_validation()\n",
        "nb_training_accuracy = cv.evaluate(training_data,training_label,10)\n",
        "end = time.time()\n",
        "nb_training_time = time.strftime(\"%H:%M:%S\", time.gmtime(end-start))\n",
        "print('Training Accuracy:',nb_training_accuracy)\n",
        "print('Training time:',nb_training_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 74.40333333333332\n",
            "Training time: 00:05:00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQi7nDb7vBW0"
      },
      "source": [
        "Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2si-M6CTu9Gn"
      },
      "source": [
        "start = time.time()\n",
        "training_data = training_data[:2000]\n",
        "nb_testing_predictions = naive_bayes(training_data,training_label,testing_data,testing_label)\n",
        "nb_testing_accuracy = (nb_testing_predictions == testing_label).mean() * 100\n",
        "end = time.time()\n",
        "print('Accuracy:',nb_testing_accuracy)\n",
        "print('Running time:',end-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpbw8rRH3Y5f"
      },
      "source": [
        "Data Load "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCs0V8RT3M8a",
        "outputId": "7d1ec555-c1e2-4479-91d1-1c1ed9c0b931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "training_data = data_train_std_pca\n",
        "testing_data = data_test_std_pca\n",
        "training_label = label_train\n",
        "testing_label = label_test\n",
        "\n",
        "print(training_data.shape)\n",
        "print(training_label.shape)\n",
        "print(testing_data.shape)\n",
        "print(testing_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 187)\n",
            "(30000,)\n",
            "(5000, 187)\n",
            "(2000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_Dro5uCwiWn"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjUrLVSawlxe"
      },
      "source": [
        "# Function to add bias to the input\n",
        "def add_bias(x):\n",
        "    return np.hstack((np.ones((x.shape[0],1)), x))\n",
        "\n",
        "inputs = add_bias(training_data)\n",
        "labels = training_label[:,np.newaxis]\n",
        "# print(inputs.shape)\n",
        "# print(labels.shape)\n",
        "\n",
        "# Initializing weights\n",
        "weights = np.zeros((inputs.shape[1], 1))\n",
        "# weights = np.random.random((inputs.shape[1],1))\n",
        "# print(weights.shape)\n",
        "\n",
        "# Function to calculate the probability of the model\n",
        "def probability_of_linear_model(inputs, weights):\n",
        "    z = np.dot(inputs, weights)\n",
        "    exp_z = np.exp(z)\n",
        "    p = exp_z/(1 + exp_z)\n",
        "    return p\n",
        "\n",
        "# Function to calculate the log likelihood loss which is minimized to obtain optimal weights\n",
        "def log_likelihood_loss(weights,inputs,labels):\n",
        "  n = inputs.shape[0]\n",
        "  # p = probability_of_linear_model(inputs,weights)\n",
        "  z = np.dot(inputs, weights)\n",
        "  exp_z = np.exp(z)\n",
        "  # print(exp_z)\n",
        "  # loss = -(1/n) * np.sum(labels*np.log(p) + (1-labels)*np.log(1-p))\n",
        "  loss =  (1/n)*np.sum(np.log(1.0+exp_z)+(labels*z))\n",
        "  return loss\n",
        "# loss = log_likelihood_loss(weights,inputs,labels)\n",
        "# print(loss)\n",
        "\n",
        "# Function to find the gradient\n",
        "def gradient(weights,inputs,labels):\n",
        "  n = inputs.shape[0]\n",
        "  p = probability_of_linear_model(inputs,weights)\n",
        "  gradient = (1/n) * np.dot(inputs.T, p - labels)\n",
        "  return gradient \n",
        "# grad = gradient(weights,inputs,labels)\n",
        "# print(grad)\n",
        "\n",
        "# Function to the fit the model: finding optimized weights by minimizing the loss \n",
        "def fit(inputs, labels, weights):\n",
        "    # optimal_weigths = fmin_tnc(func=log_likelihood_loss,x0=weights,fprime=gradient, args=(inputs, labels.flatten()))\n",
        "    # optimal_weigths = minimize(fun=log_likelihood_loss,x0=weights,args=(inputs, labels.flatten()),method='BFGS',jac=gradient)\n",
        "    # optimal_weigths = fmin(func=log_likelihood_loss,x0=weights,args=(inputs, labels.flatten()),xtol=1e-6,ftol=1e-6)\n",
        "    optimal_weigths = fmin_cobyla(func=log_likelihood_loss,x0=weights,cons=gradient,args=(inputs, labels.flatten()))\n",
        "    # optimal_weigths = fmin_tnc(func=log_likelihood_loss, x0=weights,approx_grad=True, args=(inputs, labels.flatten())) -> Taking time for gradient calculation\n",
        "    return optimal_weigths\n",
        "# def fit(inputs, labels, weights,niter,eta):\n",
        "#     for i in niter:\n",
        "#       loss = log_likelihood_loss(weights,inputs,labels)\n",
        "#       gradient = gradient(weights,inputs,labels)\n",
        "#       weights = weights - gradient * eta\n",
        "#     return optimal_weigths\n",
        "# weights_fit = fit(inputs,labels,weights)\n",
        "# print(weights_fit)\n",
        "\n",
        "def make_columns_for_each_class(labels,class_value):\n",
        "  labels_new = []\n",
        "  for i in range(0, len(labels)):\n",
        "    if labels[i] == class_value:\n",
        "      labels_new.append(1)\n",
        "    else:\n",
        "      labels_new.append(0)\n",
        "  return labels_new\n",
        "\n",
        "def find_weights(inputs, labels, weights):\n",
        "    unique_labels = list(set(labels.flatten()))\n",
        "    weights_list = []\n",
        "    for i in unique_labels:\n",
        "        weight_new = fit(inputs, labels, weights)\n",
        "        weights_list.append(weight_new)\n",
        "    return weights_list\n",
        "\n",
        "def predict(weights_list, x, y):\n",
        "    y_classes = list(set(y.flatten()))\n",
        "    y_hat = [0]*len(y)\n",
        "    for i in range(0, len(y_classes)):\n",
        "        y_new = make_columns_for_each_class(y, y_classes[i])\n",
        "        z = np.dot(x, weights_list[i])\n",
        "        exp_z = np.exp(z)\n",
        "        p = exp_z/(1 + exp_z)\n",
        "        for k in range(0, len(y)):\n",
        "            if y_new[k] == 1 and p[k] >= 0.5:\n",
        "                y_hat[k] = y_classes[i]\n",
        "    return y_hat\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9SOkY_Kxswm"
      },
      "source": [
        "Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6-WwEZ5xbJY",
        "outputId": "422924d5-8dc7-42ce-abff-7d9e375d4dd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "start = time.time()\n",
        "weights_list = find_weights(inputs, labels, weights)\n",
        "# print(weights_list)\n",
        "y_hat = predict(weights_list, inputs, labels)\n",
        "predicted = np.array(y_hat)[:,np.newaxis]\n",
        "# print(predicted.shape)\n",
        "lr_training_accuracy = (predicted == labels).mean() * 100 \n",
        "end = time.time()\n",
        "lr_training_time = time.strftime(\"%H:%M:%S\", time.gmtime(end-start))\n",
        "print('Training Accuracy:',lr_training_accuracy)\n",
        "print('Training time:',lr_training_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy: 88.26333333333334\n",
            "Training time: 00:07:39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaFFwTHwyXHv"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMK5YfWByYdO",
        "outputId": "c5225175-c204-4614-9e15-f6f968af5943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# start = time.time()\n",
        "# testing_data = testing_data[:2000]\n",
        "# test_inputs = add_bias(testing_data)\n",
        "# test_labels = testing_label[:,np.newaxis]\n",
        "# test_y_hat = predict(weights_list, test_inputs, test_labels)\n",
        "# test_predicted = np.array(test_y_hat)[:,np.newaxis]\n",
        "# lr_test_accuracy = (test_predicted == test_labels).mean() * 100 \n",
        "# end = time.time()\n",
        "# print('Accuracy:',lr_test_accuracy)\n",
        "# print('Running time:',end-start,'seconds')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 77.2\n",
            "Running time: 0.08029031753540039 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcmvcLd5KaQf"
      },
      "source": [
        "Prediction and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB8VFn44ANbO"
      },
      "source": [
        "start = time.time()\n",
        "test_inputs = add_bias(testing_data)\n",
        "test_labels = testing_label[:,np.newaxis]\n",
        "test_y_hat = predict(weights_list, test_inputs, test_labels)\n",
        "test_predicted = np.array(test_y_hat)[:,np.newaxis]\n",
        "lr_test_accuracy = (test_predicted == test_labels).mean() * 100 \n",
        "end = time.time()\n",
        "print('Accuracy:',lr_test_accuracy)\n",
        "print('Running time:',end-start,'seconds')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OsHcHZwJax6"
      },
      "source": [
        "Exporting data to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX4Jop2yAnE_"
      },
      "source": [
        "output = test_predicted\n",
        "with h5py.File('predicted_labels.h5','w') as H:\n",
        "  H.create_dataset('Output',data=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfD-sX_jDU8Q"
      },
      "source": [
        "Comparison between classifier algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pkiI4LxDbew",
        "outputId": "4a3bb05d-1544-417e-852c-fc36fc4801f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from astropy.table import  Table, Column\n",
        "\n",
        "data_rows = [('KNN', knn_training_accuracy, knn_training_time),\n",
        "             ('Naive Bayes', round(nb_training_accuracy,2), nb_training_time),\n",
        "             ('Logistic Regression', round(lr_training_accuracy,2), lr_training_time)]\n",
        "t = Table(rows=data_rows, names=('Algorithm', 'Training Accuracy', 'Training Time'))\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Algorithm      Training Accuracy Training Time\n",
            "------------------- ----------------- -------------\n",
            "                KNN             85.55      00:16:43\n",
            "        Naive Bayes              74.4      00:05:00\n",
            "Logistic Regression             88.26      00:07:39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73Hy7K1jDOE0"
      },
      "source": [
        "Hardware and Software Specifications\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Reference: [Specifications](https://www.thepythoncode.com/article/get-hardware-system-information-python)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJbvC4Uao4uQ",
        "outputId": "3224a4c8-3672-4292-c794-02b7997cca5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "import platform\n",
        "import psutil\n",
        "# System Info\n",
        "uname = platform.uname()\n",
        "print(f\"System: {uname.system}\")\n",
        "print(f\"Node Name: {uname.node}\")\n",
        "print(f\"Release: {uname.release}\")\n",
        "print(f\"Version: {uname.version}\")\n",
        "print(f\"Machine: {uname.machine}\")\n",
        "print(f\"Processor: {uname.processor}\")\n",
        "# CPU \n",
        "print('Coding enviroment: Google Colab')\n",
        "print(f\"Total CPU Usage: {psutil.cpu_percent()}%\")\n",
        "# Memory \n",
        "svmem = psutil.virtual_memory()\n",
        "print(f\"Total: {svmem.total} bytes\")\n",
        "print(f\"Available: {svmem.available} bytes\")\n",
        "print(f\"Used: {svmem.used} bytes\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "System: Linux\n",
            "Node Name: 69f58f51a834\n",
            "Release: 4.19.112+\n",
            "Version: #1 SMP Thu Jul 23 08:00:38 PDT 2020\n",
            "Machine: x86_64\n",
            "Processor: x86_64\n",
            "Coding enviroment: Google Colab\n",
            "Total CPU Usage: 1.3%\n",
            "Total: 13653557248 bytes\n",
            "Available: 12526338048 bytes\n",
            "Used: 8414552064 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}